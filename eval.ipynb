{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01bb1c3e",
   "metadata": {},
   "source": [
    "# Evaluación de los modelos\n",
    "\n",
    "Evaluamos los modelos con las diferentes métricas mencionadas en la práctica: \n",
    "| Métrica | Definición |\n",
    "|---|---|\n",
    "| F1-score (Fm) | Fm = 2 × (PR × RC) / (PR + RC) |\n",
    "| Sensibilidad (S) | S = TP / (TP + FN) |\n",
    "| Exactitud (Acc) | Acc = (TP + TN) / (TP + FP + FN + TN) |\n",
    "| Especificidad (SP) | SP = TN / (FP + TN) |\n",
    "| Recall (RC) | RC = TP / (TP + FN) |\n",
    "| Precisión (PR) | PR = TP / (TP + FP) |\n",
    "| Tasa de falsos negativos (FNR) | FNR = FN / (TP + FN) |\n",
    "| Tasa de falsos positivos (FPR) | FPR = FP / (FP + TN) |\n",
    "\n",
    "Para ello, usamos una función matriz que se encarga de llamar al resto de funciones que calculan las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b254650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 4\n",
      "True Positives: 3\n",
      "False Negatives: 2\n",
      "False Positives: 1\n",
      "                    Métrica     Valor\n",
      "0              Sensibilidad  0.600000\n",
      "1                 Exactitud  0.700000\n",
      "2             Especificidad  0.800000\n",
      "3                    Recall  0.600000\n",
      "4                 Precisión  0.750000\n",
      "5  Tasa de Falsos Negativos  0.400000\n",
      "6  Tasa de Falsos Positivos  0.200000\n",
      "7                  F1-score  0.666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "def calculate_statistics(y_true: np.ndarray, y_pred: np.ndarray, name: str) -> None:\n",
    "    \n",
    "    # Cálculo de los valores de la matriz de confusión\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "\n",
    "    print(f\"True Negatives: {TN}\")\n",
    "    print(f\"True Positives: {TP}\")\n",
    "    print(f\"False Negatives: {FN}\")\n",
    "    print(f\"False Positives: {FP}\")\n",
    "\n",
    "    \n",
    "    # Sensibilidad\n",
    "    S = TP / (TP + FN)\n",
    "    # Exactitud\n",
    "    Acc = (TP + TN) / (TP + FP + FN + TN)\n",
    "    # Especificidad\n",
    "    SP = TN / (FP + TN)\n",
    "    # Recall\n",
    "    RC = TP / (TP + FN)\n",
    "    # Precisión\n",
    "    PR = TP / (TP + FP)\n",
    "    # Tasa de falsos negativos\n",
    "    FNR = FN / (TP + FN)\n",
    "    # Tasa de falsos positivos\n",
    "    FPR = FP / (FP + TN)\n",
    "    # F1-score\n",
    "    Fm = 2 * (PR * RC) / (PR + RC)\n",
    "\n",
    "    # Guardamos los resultados en un DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'Métrica': ['Sensibilidad', 'Exactitud', 'Especificidad', 'Recall', 'Precisión', 'Tasa de Falsos Negativos', 'Tasa de Falsos Positivos', 'F1-score'],\n",
    "        'Valor': [S, Acc, SP, RC, PR, FNR, FPR, Fm]\n",
    "    })\n",
    "\n",
    "    # Guardar los resultados en un archivo CSV\n",
    "    results.to_csv(f'metricas_validacion_cruzada/{name}.csv', index=False)\n",
    "\n",
    "    print(results)\n",
    "\n",
    "calculate_statistics(\n",
    "    y_true=np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 0]),\n",
    "    y_pred=np.array([0, 1, 0, 0, 1, 1, 1, 0, 0, 0]),\n",
    "    name='ejemplo_resultados'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
