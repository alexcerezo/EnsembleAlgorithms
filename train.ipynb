{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ab2ab4",
   "metadata": {},
   "source": [
    "# Entrenamiento de los modelos\n",
    "\n",
    "Para ello realizamos una validación cruzada de 5 pasos con los cuatro métodos que se van a usar K-NN, SVM, Naive Bayes, Random forests. Previamente, se habían generado los csv con los conjuntos test y validación que ya han sido procesados previamente.\n",
    "\n",
    "Primero, creamos las funciones que se encargan de entrenar entrenar los modelos con los distintos métodos especificos. Es importante tener en cuenta que los conjuntos que tenemos para el entrenamiento contienen una cantidad de features distinta. Por lo que no será el mismo entrenamiento para conjuntos a los que se les ha realizado distintas transformaciones.\n",
    "\n",
    "En este caso, implementamos la función de K-Nearest Neighbors. Para esto necesitamos definir el hiperparámetro K. Tras consultarlo con Google Gemini y leer el foro r/datascience, hemos determinado que la K se obtiene por experimentación. Si bien es cierto que no hay que probar todos los valores de K posibles, ya que se recomienda usar solo los impares para que exista una preferencia arbitraria, con probar con los valores de K hasta la raíz cuadrada de N (tamaño del conjunto de entrenamiento) es suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3ef16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def KNN(train_data, test_data, model_name: str):\n",
    "    # Para separar el target del resto de features, asumimos que la última columna es el target\n",
    "    X_train = train_data.iloc[:, :-1].astype(float)\n",
    "    y_train = train_data.iloc[:, -1].astype(int)\n",
    "    X_test = test_data.iloc[:, :-1].astype(float)\n",
    "    y_test = test_data.iloc[:, -1].astype(int)\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_k = 1\n",
    "    best_model = None\n",
    "    \n",
    "    # Entrenamos el modelo KNN con k=1+2i (números impares) hasta la raíz cuadrada del número de muestras\n",
    "    for k in range(1, int(len(X_train)**0.5), 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_k = k\n",
    "            best_model = knn\n",
    "        print(f\"KNN con k={k}: Precisión = {accuracy:.4f}\")\n",
    "\n",
    "    # Guardamos el mejor modelo obtenido\n",
    "    print(f\"Mejor modelo: k={best_k}, Precisión = {best_accuracy:.4f}\")\n",
    "    \n",
    "    # Guardamos el modelo en formato pickle\n",
    "    os.makedirs(\"cross_validation_models\", exist_ok=True)\n",
    "    model_path = f\"cross_validation_models/{model_name}.pkl\"\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"Modelo guardado en: {model_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23579c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def SVM(train_data, test_data, model_name: str):\n",
    "    # Para separar el target del resto de features, asumimos que la última columna es el target\n",
    "    X_train = train_data.iloc[:, :-1].astype(float)\n",
    "    y_train = train_data.iloc[:, -1].astype(int)\n",
    "    X_test = test_data.iloc[:, :-1].astype(float)\n",
    "    y_test = test_data.iloc[:, -1].astype(int)\n",
    "\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"SVM: Precisión = {accuracy:.4f}\")\n",
    "\n",
    "    # Guardamos el modelo en formato pickle\n",
    "    os.makedirs(\"cross_validation_models\", exist_ok=True)\n",
    "    model_path = f\"cross_validation_models/{model_name}.pkl\"\n",
    "    joblib.dump(svm, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff491e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "def RF(train_data, test_data, model_name: str):\n",
    "    # Para separar el target del resto de features, asumimos que la última columna es el target\n",
    "    X_train = train_data.iloc[:, :-1].astype(float)\n",
    "    y_train = train_data.iloc[:, -1].astype(int)\n",
    "    X_test = test_data.iloc[:, :-1].astype(float)\n",
    "    y_test = test_data.iloc[:, -1].astype(int)\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Random Forest: Precisión = {accuracy:.4f}\")\n",
    "    \n",
    "    # Guardamos el modelo en formato pickle\n",
    "    os.makedirs(\"cross_validation_models\", exist_ok=True)\n",
    "    model_path = f\"cross_validation_models/{model_name}.pkl\"\n",
    "    joblib.dump(rf, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d700fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def NB(train_data, test_data, model_name: str):\n",
    "    print(\"Entrenando Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e407723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Creamos unos diccionarios para identificar los distintos datos de la validación cruzada\n",
    "transformations = {\n",
    "    \"N\" : \"norm\",\n",
    "    \"O\" : \"original\",\n",
    "    \"S\" : \"std\",\n",
    "}\n",
    "\n",
    "methods = {\n",
    "    \"K\" : KNN,\n",
    "    \"S\" : SVM,\n",
    "    \"R\" : RF,\n",
    "    \"N\" : NB,\n",
    "}\n",
    "\n",
    "def train_model(method: str, iteration: int, transformation_type: str, PCA: int): \n",
    "    # Obtenemos los conjuntos de entrenamiento y testing de la carpeta cross_validation_data en función de los parámetros\n",
    "    # Ejemplo: cross_validation_data/test1_norm_PCA80.csv o cross_validation_data/training1_std.csv\n",
    "    transformation_str = transformations[transformation_type]\n",
    "\n",
    "    # Construimos los nombres de los archivos de entrenamiento y prueba\n",
    "    if PCA > 0:\n",
    "        try:\n",
    "            test_file = f\"cross_validation_data/test{iteration}_{transformation_str}_PCA{PCA}.csv\"\n",
    "            train_file = f\"cross_validation_data/training{iteration}_{transformation_str}_PCA{PCA}.csv\"\n",
    "            model_name = f\"{methods[method].__name__}_{transformation_str}_PCA{PCA}_{iteration}\"\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Archivos con PCA{PCA} no encontrados, intentando sin PCA...\")\n",
    "            return\n",
    "    else :\n",
    "        try:\n",
    "            test_file = f\"cross_validation_data/test{iteration}_{transformation_str}.csv\"\n",
    "            train_file = f\"cross_validation_data/training{iteration}_{transformation_str}.csv\"\n",
    "            model_name = f\"{methods[method].__name__}_{transformation_str}_{iteration}\"\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Archivos sin PCA no encontrados.\")\n",
    "            return\n",
    "\n",
    "    # Guardamos los datos de los archivos en dos dataframes de pandas\n",
    "    # pd.read_csv ya interpreta la primera fila como header automáticamente\n",
    "    df_train = pd.read_csv(train_file)\n",
    "    df_test = pd.read_csv(test_file)\n",
    "    \n",
    "    # Llamamos a la función de entrenamiento correspondiente según el método\n",
    "    methods[method](df_train, df_test, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
